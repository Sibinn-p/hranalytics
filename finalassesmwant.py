# -*- coding: utf-8 -*-
"""finalassesmwant.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Atz54n5uqIW2MnjsmoYQ75RVGCGTC49N
"""

# Commented out IPython magic to ensure Python compatibility.

# import python libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt # visualizing data
# %matplotlib inline
import seaborn as sns

df=pd.read_csv('train_LZdllcl.csv')

dft=pd.read_csv('test_2umaH9m.csv')

df.shape

dft.shape

df.head()

dft.head()

df.info()

dft.info()

pd.isnull(df).sum()

pd.isnull(dft).sum()

df.duplicated().sum()

dft.duplicated().sum()

df.dropna(inplace=True)

dft.dropna(inplace=True)

def detect_outliers_iqr(data):
  Q1 = data.quantile(0.25)
  Q3 = data.quantile(0.75)
  IQR = Q3 - Q1
  lower_bound = Q1 - 1.5 * IQR
  upper_bound = Q3 + 1.5 * IQR
  outliers = data[(data < lower_bound) | (data > upper_bound)]
  return outliers
for column in df.select_dtypes(include=['number']):
  outliers = detect_outliers_iqr(df[column])
  if not outliers.empty:
    print(f"Outliers in '{column}': {outliers.tolist()}")

plt.figure(figsize=(15, 10))
for i, column in enumerate(df.select_dtypes(include=['number']).columns):
  plt.subplot(3, 3, i + 1)  # Adjust the subplot grid as needed
  sns.boxplot(x=df[column])
  plt.title(column)
plt.tight_layout()
plt.show()

plt.figure(figsize=(15, 10))
for i, column in enumerate(df.select_dtypes(include=['number']).columns):
  plt.subplot(3, 3, i + 1)  # Adjust the subplot grid as needed
  sns.boxplot(x=df[column])
  plt.title(column)
plt.tight_layout()
plt.show()

plt.figure(figsize=(15, 10))
for i, column in enumerate(df.select_dtypes(include=['number']).columns):
  plt.subplot(3, 3, i + 1)  # Adjust the subplot grid as needed
  sns.boxplot(x=df[column])
  plt.title(column)
plt.tight_layout()
plt.show()

df.columns

dft.columns

ax = sns.countplot(x = 'gender',data = df)

for bars in ax.containers:
    ax.bar_label(bars)

ax = sns.countplot(data = df, x = 'education', hue = 'gender')

for bars in ax.containers:
    ax.bar_label(bars)

# Commented out IPython magic to ensure Python compatibility.
# prompt: Modeling

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt # visualizing data
import seaborn as sns

# import python libraries

# %matplotlib inline



# Gender distribution
plt.figure(figsize=(8, 6))  # Adjust figure size for better visualization
ax = sns.countplot(x='gender', data=df)
ax.set_title('Distribution of Gender') # Add a title
ax.set_xlabel('Gender') # Add x-axis label
ax.set_ylabel('Count') # Add y-axis label
for bars in ax.containers:
    ax.bar_label(bars)
plt.show()

# Education level distribution by gender
plt.figure(figsize=(10, 6))  # Adjust figure size
ax = sns.countplot(data=df, x='education', hue='gender')
ax.set_title('Distribution of Education Levels by Gender') # Add a title
ax.set_xlabel('Education Level') # Add x-axis label
ax.set_ylabel('Count') # Add y-axis label
ax.tick_params(axis='x', rotation=45) # Rotate x-axis labels if needed
for bars in ax.containers:
    ax.bar_label(bars)
plt.show()

# prompt: min max scaling

from sklearn.preprocessing import MinMaxScaler

# Assuming 'df' is your DataFrame and you want to scale numerical features
numerical_cols = df.select_dtypes(include=['number']).columns

# Initialize the MinMaxScaler
scaler = MinMaxScaler()

# Fit and transform the numerical columns
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])
# Display the scaled DataFrame
print(df.head())

# prompt: find the best model of df

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Assuming 'df' is your DataFrame and 'target_column' is the name of the target variable column

# Separate features (X) and target variable (y)
X = df.drop('is_promoted', axis=1)  # Replace 'target_column' with the actual name
y = df['is_promoted']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train different models
models = {
    'Logistic Regression': LogisticRegression(),
    'Random Forest': RandomForestClassifier(),
    'SVM': SVC()
}

results = {}
for name, model in models.items():

    accuracy = accuracy_score(y_test, y_pred)
    results[name] = accuracy
    print(f"{name} accuracy: {accuracy}")


# Find the best model
best_model = max(results, key=results.get)
print(f"\nThe best model is: {best_model} with accuracy {results[best_model]}")



# : fine tubbing


# Fine-tuning visualizations

# Gender distribution
plt.figure(figsize=(8, 6))
ax = sns.countplot(x='gender', data=df)
ax.set_title('Distribution of Gender', fontsize=14)  # Increased font size
ax.set_xlabel('Gender', fontsize=12)
ax.set_ylabel('Count', fontsize=12)
ax.tick_params(axis='both', labelsize=10) # Adjust tick label size

for p in ax.patches:
    ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),
                textcoords='offset points')  # Improved annotation placement

plt.show()


# Education level distribution by gender
plt.figure(figsize=(12, 6))  # Wider figure for better label readability
ax = sns.countplot(data=df, x='education', hue='gender')
ax.set_title('Distribution of Education Levels by Gender', fontsize=14)
ax.set_xlabel('Education Level', fontsize=12)
ax.set_ylabel('Count', fontsize=12)
ax.tick_params(axis='x', rotation=45, labelsize=10)  # Rotate and adjust label size
ax.tick_params(axis='y', labelsize=10)

for p in ax.patches:
    ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),
                textcoords='offset points')


plt.tight_layout() # Adjust layout to prevent labels from overlapping
plt.show()

#  Replace the target column with the prediction value

# Assuming 'df' is your DataFrame and you want to scale numerical features
numerical_cols = df.select_dtypes(include=['number']).columns

# Initialize the MinMaxScaler
scaler = MinMaxScaler()

# Fit and transform the numerical columns
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])

# Assuming 'df' is your DataFrame and 'target_column' is the name of the target variable column

# Separate features (X) and target variable (y)
X = df.drop('is_promoted', axis=1)  # Replace 'target_column' with the actual name
y = df['is_promoted']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)





# Replace the target column in the test set with the prediction values
X_test['is_promoted'] = y_pred

#Now X_test contains the replaced target column
print(X_test.head())

# prompt: X_test  download as csv only employ id andis promoted

# Assuming X_test is already defined as in your provided code
# and y_pred contains the predictions

# Create a new DataFrame with only 'employee_id' and 'is_promoted'
X_test_subset = X_test[['employee_id', 'is_promoted']].copy()

# Convert the DataFrame to a CSV file
X_test_subset.to_csv('X_test_predictions.csv', index=False)

# Download the CSV file
from google.colab import files
files.download('X_test_predictions.csv')

# Save this newdataset as a csv file  in devic

# Assuming 'X_test' is the DataFrame you want to save
from google.colab import files
X_test.to_csv('newdataset.csv', encoding = 'utf-8-sig')
files.download('newdataset.csv')

